{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"A2.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"u_teWmHi_H6o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621266065140,"user_tz":240,"elapsed":1303,"user":{"displayName":"Yiyun Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHHU7PrGLYz-yThrTFqGwYryQ3cvybJgTC2WOftA=s64","userId":"11418697320007205677"}},"outputId":"4e9ed55a-4097-4e81-e9c2-f0623e24f010"},"source":["## Imports all necessary modules\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from numpy import linalg\n","from scipy import stats\n","from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n","from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import preprocessing\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"jSBf1jtD_H6u","colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"status":"error","timestamp":1621266065525,"user_tz":240,"elapsed":1626,"user":{"displayName":"Yiyun Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHHU7PrGLYz-yThrTFqGwYryQ3cvybJgTC2WOftA=s64","userId":"11418697320007205677"}},"outputId":"a19276a5-ffce-4a08-d28a-c03d04631c69"},"source":["## This loads the csv file from disk\n","yelp_data = pd.read_csv(\n","    filepath_or_buffer = \"./data/Yelp_Usefulness_Assignment2_1.csv\", sep = \",\", header=0 )\n","\n","print(yelp_data.head(20))\n","\n","## Print the dimension of the data\n","print(yelp_data.shape)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d7483b67c80c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## This loads the csv file from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m yelp_data = pd.read_csv(\n\u001b[0;32m----> 3\u001b[0;31m     filepath_or_buffer = \"./data/Yelp_Usefulness_Assignment2_1.csv\", sep = \",\", header=0 )\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myelp_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/Yelp_Usefulness_Assignment2_1.csv'"]}]},{"cell_type":"code","metadata":{"id":"5dTwouHU_H6v"},"source":["## Applying descriptive analysis\n","pd.set_option('display.max_columns', None)\n","\n","print(yelp_data.describe())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-9tJGkiN_H6v"},"source":["print(yelp_data[[\"review_id\", \"class\"]].describe())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wfm5xKGY_H6v"},"source":["### 1\n","###### 1.1\n","###### 1.1.1"]},{"cell_type":"markdown","metadata":{"id":"VBaVj-QM_H6w"},"source":["The first problem is missing values, we can use isna() function to detect missing values in each attribute."]},{"cell_type":"code","metadata":{"id":"7PXDmNJp_H6w"},"source":["## Show the number of data per attributes that has missing values\n","print(yelp_data.isna().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vyfl5Owu_H6w"},"source":["The report shows there are 5 missing values in the \"eigenvector\" attribute, then uses the .isna() function again to check the missing values across the tuples."]},{"cell_type":"code","metadata":{"id":"064Md-TV_H6x"},"source":["## Check the missing values across tuples of the \"eigenvector\" attribute\n","print(yelp_data[\"eigenvector\"].isna())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GBJjEoHk_H6x"},"source":["###### 1.1.2"]},{"cell_type":"code","metadata":{"id":"RYipXtxy_H6x"},"source":["## Create box plot to see the distribution of the \"eigenvector\" attribute\n","fig = plt.figure(figsize = (6, 6))\n","\n","plt.boxplot(\n","    yelp_data[\"eigenvector\"][~yelp_data[\"eigenvector\"].isna()], labels = [\"Eigenvector\"]\n",")\n","\n","plt.tick_params(labelsize = 15)\n","\n","plt.title(\"The normalized number of words related to 'eigenvector' sentiment\", fontsize = 15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7GwDIPoX_H6y"},"source":["It seems most values are not zero, and it is negatively skewed. We can use the median central tendency."]},{"cell_type":"code","metadata":{"id":"C0T5DheB_H6y"},"source":["## Exclude missing tuples to get median\n","median = np.median(yelp_data[\"eigenvector\"][~yelp_data[\"eigenvector\"].isna()])\n","print (\"The median is: \", median)\n","\n","## Replace with median \n","yelp_data[\"eigenvector\"].fillna(median, inplace=True)\n","\n","## Check filled attributes\n","print(yelp_data.isna().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X3l-p8h5_H6y"},"source":["###### 1.1.3"]},{"cell_type":"markdown","metadata":{"id":"CE5qrIzl_H6y"},"source":["This approach's pro is not abandoning any data and it has a good chance to hit or close to the actual values. However, the con is choosing median doesn't necessarily give us the advantage if the missing values are evenly or uniform distributed."]},{"cell_type":"markdown","metadata":{"id":"RHz06zpr_H6y"},"source":["###### 1.2\n","###### 1.2.1"]},{"cell_type":"markdown","metadata":{"id":"tQQ65xy3_H6z"},"source":["The second problem is outliers, we can use Z-score for outlier detection."]},{"cell_type":"code","metadata":{"id":"UlfZjz2s_H6z"},"source":["# Use Z-score  to detect ourliers\n","positions = list(range(1,13))\n","positions.extend(list(range(14,25)))\n","\n","z = np.abs(stats.zscore(yelp_data.iloc[:, positions]))\n","print(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-wFZ8yF_H6z"},"source":["threshold = 3\n","print(np.where(z > 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tljVzZoX_H60"},"source":["unique_elements, counts_elements = np.unique(np.where(z > 3)[0], return_counts=True)\n","print(np.asarray((unique_elements, counts_elements)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ngpenHgz_H60"},"source":["[69, 262] - [6,6] is the outlier."]},{"cell_type":"markdown","metadata":{"id":"LDshTrvd_H60"},"source":["###### 1.2.2"]},{"cell_type":"markdown","metadata":{"id":"7CuCdn0Y_H60"},"source":["We can remove outliers from dataset by using drop() function."]},{"cell_type":"code","metadata":{"id":"lsXW2MkE_H60"},"source":["## Remove largest number of outliers 69, 262.\n","yelp_data.drop([69, 262])\n","\n","## Show dimension of data with removed outlier tuples, two tuples are removed.\n","print(yelp_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xcuUDcEe_H61"},"source":["###### 1.2.3"]},{"cell_type":"markdown","metadata":{"id":"2gH82Zon_H61"},"source":["The pro of this approach is it may eliminate the data errors and improve the data quality. The con of this approach is if the outliers are legitimate, the observations may not as expected and the outcomes may be wrong."]},{"cell_type":"markdown","metadata":{"id":"OZ5KisO2_H61"},"source":["###### 1.3\n","###### 1.3.1"]},{"cell_type":"markdown","metadata":{"id":"lSduSpEI_H61"},"source":["The third problem is redundancies. We can use correlation analysis to check redundancies."]},{"cell_type":"code","metadata":{"id":"CHtKcYfh_H61"},"source":["pcorr = yelp_data.corr(method='pearson')\n","\n","pd.set_option('display.max_columns', None)\n","pcorr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HlLFFTL_H62"},"source":["plt.figure(figsize=(12,10))\n","sns.heatmap(pcorr, annot=True, cmap=plt.cm.Reds)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5hoyUnF_H62"},"source":["###### 1.3.2"]},{"cell_type":"markdown","metadata":{"id":"7hRQ4kfs_H62"},"source":["We can drop one attribute inside the strongest correlation, which is 0.94 between degree and eigenvector."]},{"cell_type":"code","metadata":{"id":"N0-e8hgW_H62"},"source":["yelp_data.drop([\"eigenvector\"], 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h7uWHbNj_H62"},"source":["###### 1.3.3"]},{"cell_type":"markdown","metadata":{"id":"UyrnyQ1l_H63"},"source":["The pro of this approach is to reduce the redundancies of the data. The con is abandoned one attribute that may have a significant impact on observation and processing."]},{"cell_type":"markdown","metadata":{"id":"6D33mesG_H63"},"source":["### 2"]},{"cell_type":"code","metadata":{"id":"KvzOx91g_H63"},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","## This loads the csv file from disk\n","yelp_data = pd.read_csv(\n","    filepath_or_buffer = \"./data/Yelp_Usefulness_Assignment2_2.csv\", sep = \",\", header=0 )\n","\n","print(yelp_data.head(20))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kG0dH5t0_H63"},"source":["## Create feature matrix by dropping the review_id and label attribute\n","## Review_id is not going to helpful to predict the usefulness of reviews\n","X = yelp_data.drop([\"review_id\",\"class\"], 1)    \n","\n","## Pre-processing. Sklearn takes integer as label\n","## Create target attribute\n","yelp_data[yelp_data['class'] == 'useful'] = 1\n","yelp_data[yelp_data['class'] == 'not_useful'] = 0\n","\n","## Specify the data type. Before specifying, the type was unknown\n","y = yelp_data[\"class\"].astype('int')\n","\n","## Create a model\n","clf = LogisticRegression()\n","clf.fit(X, y)\n","\n","## predict target class based on the trained model \n","predictions = clf.predict(X)\n","\n","## Calculate the performance of the classifier\n","accuracy = accuracy_score(predictions, y)\n","\n","print(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZMfhqcO_H63"},"source":["###### 2.1"]},{"cell_type":"markdown","metadata":{"id":"IGHNP812_H64"},"source":["The best accuracy I got is 0.762."]},{"cell_type":"code","metadata":{"id":"vqWv8vZq_H64"},"source":["## import the necessary libraries\n","from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n","\n","## Apply z-transformation\n","z_scaler = preprocessing.StandardScaler()\n","X_scaled = z_scaler.fit_transform(X)\n","X_scaled = pd.DataFrame(X_scaled, columns = X.columns)\n","\n","## Sequential Forward Selection(sfs)\n","sfs = SFS(LogisticRegression(),\n","           k_features=(1,X_scaled.shape[1]),\n","           forward=True, \n","           floating=True,\n","           scoring = 'accuracy',\n","           cv = 0)\n","\n","sfs = sfs.fit(X_scaled, y)\n","## Get the final set of features\n","print(sfs.k_feature_names_)\n","\n","X_selected = sfs.transform(X_scaled)\n","\n","# Fit the estimator using the new feature subset\n","# and make a prediction on the test data\n","clf.fit(X_selected, y)\n","predictions = clf.predict(X_selected)\n","\n","## Calculate the performance of the classifier\n","accuracy = accuracy_score(predictions, y)\n","\n","print(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VvuZHAHn_H64"},"source":["###### 2.2"]},{"cell_type":"markdown","metadata":{"id":"X1Bou0uD_H64"},"source":["I have applied z-transformation and Min_Max Scaler with different classifier options:  k_features=(1,X_scaled.shape[1])/2/3, foward=True/False, floating=True/False, scoring='accuracy'/'neg_mean_squared_error', cv=0/5/10.\\\n","The overall performance of Min_Max Scaler's accuracy is lower than z-transformation, no matter how I change the arguments in classifiers. The Min_Max Scaler's accuracy is always lower than the baseline classifier 0.752, even set cv to 0.\\\n","The cv is cross-validation schemes. When cv increases, the accuracy is lower and it takes longer to execute. When cv is 0, the other classifiers' arguments no longer affect the accuracy.\\\n","In z-transformation, set forward from False to True, the accuracy decrease.\\\n","In z-transformation, set floating from False to True, the accuracy increase.\\\n","In Min_Max Scaler, set forward from False to True, the accuracy increase.\\\n","In Min_Max Scaler, set floating from False to True, the accuracy increase."]},{"cell_type":"markdown","metadata":{"id":"AnFQzECN_H65"},"source":["### 3\n","###### 3.1\n","###### 3.1.1"]},{"cell_type":"markdown","metadata":{"id":"69OeVno2FTYv"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"yOkjTw0g_H65"},"source":["Frequent 1-itemsets"]},{"cell_type":"markdown","metadata":{"id":"1Bw5nVAA_H65"},"source":["|Itemset|Count|Total # of Transactions|Support|Passing Minimum Support|\n","|------|------|------|------|------|\n","|Computer|3|6|3/6=0.5|Yes|\n","|Mouse|2|6|2/6=0.3|No|\n","|Smart Watch|4|6|4/6=0.67|Yes|\n","|Tablet|2|6|2/6=0.3|No|\n","|Smart Phone|3|6|3/6=0.5|Yes|\n","|Game Console|2|6|2/6=0.3|No|"]},{"cell_type":"markdown","metadata":{"id":"o7E0hL6V_H65"},"source":["Frequent 2-itemsets"]},{"cell_type":"markdown","metadata":{"id":"oWv9AYog_H65"},"source":["|Itemset|Count|Total # of Transactions|Support|Passing Minimum Support|\n","|------|------|------|------|------|\n","|Computer, Smart Watch|1|6|1/6=0.17|No|\n","|Computer, Smart Phone|1|6|1/6=0.17|No|\n","|Smart Watch, Smart Phone|3|6|3/6=0.5|Yes|"]},{"cell_type":"markdown","metadata":{"id":"fqbJTQQv_H66"},"source":["Frequent 3-itemsets"]},{"cell_type":"markdown","metadata":{"id":"aXlZl5k5_H66"},"source":["|Itemset|Count|Total # of Transactions|Support|Passing Minimum Support|\n","|------|------|------|------|------|\n","|Computer, Smart Watch, Smart Phone|1|6|1/6=0.17|No|"]},{"cell_type":"markdown","metadata":{"id":"cPG0-teC_H66"},"source":["###### 3.1.2"]},{"cell_type":"markdown","metadata":{"id":"-fxIlb6D_H66"},"source":["|Rule|support (A⇒B)|confidence (A⇒B)|Passing Minimum Support|Passing Minimum Confidence|\n","|------|------|------|------|------|\n","|Smart Watch ⇒ Smart Phone|3/6=0.5|3/4=0.75|Yes|Yes|\n","|Smart Phone ⇒ Smart Watch|3/6=0.5|3/3=1|Yes|Yes|"]},{"cell_type":"markdown","metadata":{"id":"0XfekCx9_H66"},"source":["###### 3.2\n","###### 3.2.1"]},{"cell_type":"markdown","metadata":{"id":"4EmqADhQ_H67"},"source":["Expected Table"]},{"cell_type":"markdown","metadata":{"id":"QAGFphPe_H67"},"source":["||Smart Watch|Not Smart Watch|\n","|---|---|---|\n","|Smart Phone|500(850*0.6=510)|350(850*0.4=340)|\n","|Not Smart Phone|100(150*0.6=90)|50(150*0.4=60)|"]},{"cell_type":"markdown","metadata":{"id":"oBgY8EkE_H67"},"source":["|Rule|support (A⇒B)|confidence (A⇒B)|Lift (A⇒B)|chi-squared ( χ2 )|\n","|------|------|------|------|------|\n","|Smart Watch ⇒ Smart Phone|500/1000=0.5|500/600=0.83|(500/1000)/((850/1000)*(600/1000))=0.98|(500-510)^2/500+(100-90)^2/100+(500-510)^2/500+(350-340)^2/350=1.69|\n","|Not Smart Watch ⇒ Smart Phone|350/1000=0.35|350/400=0.875|(350/1000)/((400/1000)*(850/1000))=1.03|(350-340)^2/350+(50-60)^2/50+(500-510)^2/500+(350-340)^2/350=2.77|"]},{"cell_type":"markdown","metadata":{"id":"VnTotStS_H67"},"source":["###### 3.2.2"]},{"cell_type":"markdown","metadata":{"id":"3t1anIkL_H67"},"source":["Both support and confidence values are larger than 0.35, which means they are reliable rules. Smart Watch ⇒ Smart Phone's lift is lower than 1, it means A and B are negatively correlated. Not Smart Watch ⇒ Smart Phone's lift is larger than 1, it means A and B are positively correlated. chi-squared shows Smart Watch and Smart Phone are negatively related because 510 was expected but only have 500, the Not Smart Watch and Smart Phone are positively related because 90 was expected but have 100. Support and confidence can show if rules are reliable but cannot tell their correlation. Lift can easily calculate the correlation but cannot tell too much difference. Chi-squared is hard to calculate but it shows the correlation clearly. In the future, I will still make a table like this to examine the rules because lift and chi-squared can cross-validate the conclusion."]},{"cell_type":"markdown","metadata":{"id":"YJ3i9sWY_H67"},"source":["### 4\n","###### 4.1"]},{"cell_type":"code","metadata":{"id":"fNGnj4Cy_H68","executionInfo":{"status":"error","timestamp":1623058381920,"user_tz":240,"elapsed":260,"user":{"displayName":"Yiyun Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-1s5MOyCBTKm2ULsi5H7t3YQmPQay9kVXE3pXZg=s64","userId":"11418697320007205677"}},"outputId":"e2e3c626-ab5c-492f-a23f-1a764fad14a3","colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["pd.options.mode.chained_assignment = None  # default='warn'\n","\n","## This loads the csv file from disk\n","yelp_data = pd.read_csv(\n","    filepath_or_buffer = \"./data/Yelp_Usefulness_Assignment2_2.csv\", sep = \",\", header=0 )\n","\n","print(yelp_data.head(20))\n","print(yelp_data.shape)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0c6e2959a9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchained_assignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# default='warn'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## This loads the csv file from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m yelp_data = pd.read_csv(\n\u001b[1;32m      5\u001b[0m     filepath_or_buffer = \"./data/Yelp_Usefulness_Assignment2_2.csv\", sep = \",\", header=0 )\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"code","metadata":{"id":"88l51VzB_H68","executionInfo":{"status":"aborted","timestamp":1623058381916,"user_tz":240,"elapsed":5,"user":{"displayName":"Yiyun Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-1s5MOyCBTKm2ULsi5H7t3YQmPQay9kVXE3pXZg=s64","userId":"11418697320007205677"}}},"source":["yelp_data = yelp_data.drop([\"review_id\"], 1)\n","yelp_data['class'][yelp_data['class'] == 'useful'] = 1\n","yelp_data['class'][yelp_data['class'] == 'not_useful'] = 0\n","yelp_data[\"class\"].astype('int')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVIYjOqd_H68","executionInfo":{"status":"aborted","timestamp":1623058381918,"user_tz":240,"elapsed":6,"user":{"displayName":"Yiyun Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-1s5MOyCBTKm2ULsi5H7t3YQmPQay9kVXE3pXZg=s64","userId":"11418697320007205677"}}},"source":["for attribute in X:\n","    yelp_data[attribute] = np.where(yelp_data[attribute] >= np.mean(yelp_data[attribute]),1,0)\n","    yelp_data[attribute].astype('int')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ECp_Cve_H68","executionInfo":{"status":"aborted","timestamp":1623058381918,"user_tz":240,"elapsed":6,"user":{"displayName":"Yiyun Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-1s5MOyCBTKm2ULsi5H7t3YQmPQay9kVXE3pXZg=s64","userId":"11418697320007205677"}}},"source":["print(yelp_data.head(20))\n","print(yelp_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2jZWl4ea_H68"},"source":["###### 4.2"]},{"cell_type":"code","metadata":{"id":"yU39bYpp_H69","executionInfo":{"status":"aborted","timestamp":1623058381919,"user_tz":240,"elapsed":7,"user":{"displayName":"Yiyun Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-1s5MOyCBTKm2ULsi5H7t3YQmPQay9kVXE3pXZg=s64","userId":"11418697320007205677"}}},"source":["from mlxtend.frequent_patterns import apriori, association_rules\n","\n","frequent_itemsets = apriori(yelp_data, min_support=0.3, use_colnames=True)\n","\n","print(frequent_itemsets.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9_MZHN4V_H69","executionInfo":{"status":"aborted","timestamp":1623058381919,"user_tz":240,"elapsed":7,"user":{"displayName":"Yiyun Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-1s5MOyCBTKm2ULsi5H7t3YQmPQay9kVXE3pXZg=s64","userId":"11418697320007205677"}}},"source":["rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n","print(rules)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_Lrs8uO_H69","executionInfo":{"status":"aborted","timestamp":1623058381920,"user_tz":240,"elapsed":8,"user":{"displayName":"Yiyun Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-1s5MOyCBTKm2ULsi5H7t3YQmPQay9kVXE3pXZg=s64","userId":"11418697320007205677"}}},"source":["rules[ (rules['lift'] > 1) & (rules['confidence'] >= 0.3) ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pCsB-cPl_H69"},"source":["###### 4.3"]},{"cell_type":"markdown","metadata":{"id":"yCK5CP-T_H69"},"source":["Since lift equals to 1 implies antecedent and consequent are independent, while it produces too many lines, I am using rules['lift'] > 1 as the filter. There are 57 rules that are positively correlated.\n","One interesting rule is (correct_spell_ratio) ⇒ (FleschReadingEase), it logically makes sense and the lift evaluation measure is larger than 1 which means they are positively correlated.\n","One not interesting rule is (dislike, review_stars)\t⇒ (correct_spell_ratio), the (correct_spell_ratio) should be the antecedents of result (dislike, review_stars), but the evaluation measure lift shows they are positively correlated, although it is very close to 1.\n","However, from this exercise, I learned how to find association rules from a dataset. The outcome rules are make sense, antecedents and consequents are positively correlated, from both logical and mathematical perspective."]},{"cell_type":"markdown","metadata":{"id":"T2Rs6bJwFW3G"},"source":["    Q3.1.2 \n","    \n"," \n","    \n"," \n","    \n"," \n","     Incorrect support scores; please compare them to those you have in Q3.1.2 and Q3.2. Better to list your calculation steps. (-0.1) \n","    \n"," \n","    \n"," \n","    \n"," \n","     # 3.2.1 \n","    \n"," \n","    \n"," \n","    \n"," \n","     Chi-squared scores for both cases should be the same (around 3.27). (-0.1) \n","    \n"," \n","    \n"," \n","    \n"," \n","     # 3.2.2 \n","    \n"," \n","    \n"," \n","    \n"," \n","     Both Chi-squared (less than critical value 3.84) and Lift (around 1) scores indicate that the two are rather indpendent. (-0.2) "]}]}