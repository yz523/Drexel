{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of WordCount_Demo.ipynb","provenance":[{"file_id":"14fZzB4pK4zSXFtjNAwl2qOFzp0MGd1G9","timestamp":1619898931724}],"collapsed_sections":[],"authorship_tag":"ABX9TyMe7Y370vYA9QnvOzU8O7Qy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7-11U_Fg1O-8"},"source":["### WordCount program using PySpark"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Epzkgj2O1HE5","executionInfo":{"status":"ok","timestamp":1619897214271,"user_tz":240,"elapsed":3184,"user":{"displayName":"Il-Yeol Song","photoUrl":"","userId":"10043542795749421186"}},"outputId":"d9d87385-3982-4c69-b259-685c12dcd426"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.1)\n","Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"SbWQWqVn1YMP","executionInfo":{"status":"ok","timestamp":1619897249751,"user_tz":240,"elapsed":141,"user":{"displayName":"Il-Yeol Song","photoUrl":"","userId":"10043542795749421186"}},"outputId":"52c7f866-6945-47c2-a056-a6ad8e164ad5"},"source":["from pyspark.sql import SparkSession\n","APP_NAME = \"WC_Example\"\n","spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n","spark"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://95ad221bfa42:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>WC_Example</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f9783ea9890>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"9fv15ZSr8qjM"},"source":["#import library for sql functions\n","import pyspark.sql.functions as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hrfq1Wr_1jme","executionInfo":{"status":"ok","timestamp":1619897258838,"user_tz":240,"elapsed":411,"user":{"displayName":"Il-Yeol Song","photoUrl":"","userId":"10043542795749421186"}},"outputId":"16100f99-29ad-407c-f781-e3e5e59066a5"},"source":["#Connect with gdrive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CERAQEF-11JJ"},"source":["# Specify the data file to count"]},{"cell_type":"code","metadata":{"id":"15rap4Om18hn"},"source":["#import files from the Web\n","import requests\n","response = requests.get('https://www.gutenberg.org/files/1342/1342-0.txt')\n","if response.ok:\n","    with open('pride_and_prejudice.txt', 'w', encoding='utf-8') as file:\n","        file.writelines(response.text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fpeh1762Xu_"},"source":["# load Pride and Prejudice into a DataFrame\n","wordDF = spark.read.text('pride_and_prejudice.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SccME7Exfse6","executionInfo":{"status":"ok","timestamp":1619898895683,"user_tz":240,"elapsed":2313,"user":{"displayName":"Il-Yeol Song","photoUrl":"","userId":"10043542795749421186"}},"outputId":"fc346838-4e82-417e-ae17-6100fc6f8688"},"source":["# WordCount using DF (Using space)\n","# split() converts delimiter separated String to array on Dataframe, based on a delimiter like space, comma, etc\n","# explode() flatten nested array (Array of Array) DataFrame columns into rows\n","# selectExpr() takes SQL expression in a String and returns a new DataFrame\n","# Split, trim, and count the words.  Display in descending count order\n","wc = (wordDF.select(F.split('value',' ').alias('words'))    # split words into an array\n","         .select(F.explode('words').alias('word'))             # make each item of the array its own line\n","         .selectExpr(\"trim(word) as word\")                     # left and right trim\n","         .filter(\"word != ''\")                                 # filter out the empty strings\n","         .groupBy(['word'])                                    # group by word\n","         .agg(F.count('word'))                                 # perform a count aggregation on each word\n","         .orderBy(F.desc('count(word)')))                      # order descending\n","    \n","wc.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----+-----------+\n","|word|count(word)|\n","+----+-----------+\n","| the|       4216|\n","|  to|       4123|\n","|  of|       3667|\n","| and|       3309|\n","|   a|       1944|\n","| her|       1856|\n","|  in|       1817|\n","| was|       1796|\n","|   I|       1725|\n","|that|       1417|\n","| not|       1363|\n","| she|       1303|\n","|  be|       1209|\n","| his|       1166|\n","| had|       1125|\n","|  as|       1119|\n","|with|       1040|\n","|  he|       1039|\n","| for|       1003|\n","| you|        992|\n","+----+-----------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eiGvTRic5xwB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-3zmJ3E5zEJ"},"source":["# WordCount program using RDD"]},{"cell_type":"code","metadata":{"id":"fPuyBKAgZEGL"},"source":["# load Pride and Prejudice into an RDD\n","wRDD = spark.sparkContext.textFile('pride_and_prejudice.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xldhR_CGZS2H","executionInfo":{"status":"ok","timestamp":1619898553053,"user_tz":240,"elapsed":1128,"user":{"displayName":"Il-Yeol Song","photoUrl":"","userId":"10043542795749421186"}},"outputId":"18dfb372-0897-4097-f144-7fb56e9c113a"},"source":["#Word Count program using RDD \n","wcRDD = (wRDD.flatMap(lambda line: str(line).split(' '))\n","            .map(lambda word: (word, 1))\n","            .reduceByKey(lambda v1,v2: v1+v2)\n","            .sortBy((lambda x: x[1]), False))\n","wcRDD.take(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('', 73700),\n"," ('the', 4216),\n"," ('to', 4123),\n"," ('of', 3667),\n"," ('and', 3309),\n"," ('a', 1944),\n"," ('her', 1856),\n"," ('in', 1817),\n"," ('was', 1796),\n"," ('I', 1725),\n"," ('that', 1417),\n"," ('not', 1363),\n"," ('she', 1303),\n"," ('be', 1209),\n"," ('his', 1166),\n"," ('had', 1125),\n"," ('as', 1119),\n"," ('with', 1040),\n"," ('he', 1039),\n"," ('for', 1003)]"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7_IO5U09ZhN","executionInfo":{"status":"ok","timestamp":1619890156516,"user_tz":240,"elapsed":168,"user":{"displayName":"Il-Yeol Song","photoUrl":"","userId":"10043542795749421186"}},"outputId":"3861843e-5c58-4a2b-8870-a1815f6e7243"},"source":["# Find file names in gdrive\n","import os\n","os.listdir('./gdrive/My Drive/Asllani/ch11_data')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['myBlog.txt',\n"," 'SalesRecords.csv',\n"," 'EmployeePT.tsv',\n"," 'customerFiles.json',\n"," '.DS_Store',\n"," 'items']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3kDD7yxX5yZ1","executionInfo":{"status":"ok","timestamp":1619895807022,"user_tz":240,"elapsed":337,"user":{"displayName":"Il-Yeol Song","photoUrl":"","userId":"10043542795749421186"}},"outputId":"b2662fdc-ae93-4cf5-c141-8b25756df6ec"},"source":["# Code for reading a text file from gdrive into wfRDD\n","wordRDD = spark.sparkContext.textFile(\"./gdrive/My Drive/Asllani/ch11_data/myBlog.txt\")\n","wordRDD.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Big Data techniques offer several advantages over traditional techniques.',\n"," 'Hadoop is one Big Data analytics platform.  Hadoop has several ecosystems to perform Big Data Analytics.',\n"," 'Spark is one of such ecosystems.  Spark can also run on Hadoop for Big Data Analitycs.']"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0M0GcYbl0--H","executionInfo":{"status":"ok","timestamp":1619895811894,"user_tz":240,"elapsed":1160,"user":{"displayName":"Il-Yeol Song","photoUrl":"","userId":"10043542795749421186"}},"outputId":"63097322-46a7-407f-fd38-2ee34ea6fd4e"},"source":["#Word Count program using RDD (DF has no flatMap operation)\n","wc = (wordRDD.flatMap(lambda line: str(line).split(' '))\n","            .map(lambda word: (word, 1))\n","            .reduceByKey(lambda v1,v2: v1+v2)\n","            .sortBy((lambda x: x[1]), False))\n","wc.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Big', 4),\n"," ('Data', 4),\n"," ('Hadoop', 3),\n"," ('several', 2),\n"," ('is', 2),\n"," ('', 2),\n"," ('Spark', 2),\n"," ('one', 2),\n"," ('techniques', 1),\n"," ('advantages', 1),\n"," ('traditional', 1),\n"," ('analytics', 1),\n"," ('platform.', 1),\n"," ('ecosystems', 1),\n"," ('perform', 1),\n"," ('Analytics.', 1),\n"," ('of', 1),\n"," ('run', 1),\n"," ('offer', 1),\n"," ('over', 1),\n"," ('techniques.', 1),\n"," ('has', 1),\n"," ('to', 1),\n"," ('such', 1),\n"," ('ecosystems.', 1),\n"," ('can', 1),\n"," ('also', 1),\n"," ('on', 1),\n"," ('for', 1),\n"," ('Analitycs.', 1)]"]},"metadata":{"tags":[]},"execution_count":26}]}]}